# üï∑Ô∏è AI-Powered Search Engine - Web Crawler

A comprehensive, production-ready web crawler that extracts and indexes web content into Elasticsearch with support for incremental crawling, multiple content types, and intelligent content extraction.

## üåü Features

### **Core Functionality**
- ‚úÖ **URL List & Sitemap Support** - Crawl from direct URLs, URL files, or XML sitemaps
- ‚úÖ **Content Extraction** - Extract title, URL, meta description, content snippet, and keywords
- ‚úÖ **Text Normalization** - Clean and normalize extracted text content
- ‚úÖ **Incremental Crawling** - Only process new or updated pages using content hashing
- ‚úÖ **Multiple Content Types** - Support for news, blogs, e-commerce, documentation, videos, and academic content

### **Advanced Features**
- ü§ñ **Robots.txt Compliance** - Respect website crawling policies
- ‚ö° **Concurrent Processing** - Configurable parallel crawling with rate limiting
- üîÑ **Retry Logic** - Automatic retry with exponential backoff for failed requests
- üìä **Progress Monitoring** - Real-time crawling statistics and progress tracking
- üéØ **Smart Content Detection** - Automatic content type and language detection
- üõ°Ô∏è **Error Handling** - Comprehensive error handling and logging

### **Content Extraction**
- **Title**: Page title, H1, or Open Graph title
- **URL**: Canonical URL with deduplication
- **Snippet**: Meta description or first paragraph (300 chars)
- **Keywords**: Meta keywords, headings, and emphasized text
- **Content**: Main article content with noise removal
- **Source**: Domain and content type classification
- **Metadata**: Author, publish date, language, reading time

## üöÄ Quick Start

### Installation

```bash
# Navigate to crawler directory
cd scripts/crawler

# Install dependencies
npm install

# Make executable
chmod +x web-crawler.js
```

### Basic Usage

```bash
# Crawl a single URL
node web-crawler.js "https://example.com"

# Crawl multiple URLs
node web-crawler.js "https://site1.com" "https://site2.com" "https://site3.com"

# Crawl from sitemap
node web-crawler.js "https://example.com/sitemap.xml"

# Crawl from URL file
node web-crawler.js --file urls.txt
```

### Environment Configuration

Create a `.env` file in the crawler directory:

```bash
# Elasticsearch Configuration
ELASTICSEARCH_URL=http://localhost:9200
ELASTICSEARCH_API_KEY=your-api-key
CRAWLER_INDEX=web-content

# Crawler Settings
CRAWLER_CONCURRENT=5          # Max concurrent requests
CRAWLER_DELAY=1000           # Delay between requests (ms)
CRAWLER_TIMEOUT=30000        # Request timeout (ms)
CRAWLER_RETRIES=3            # Max retry attempts
CRAWLER_MAX_PAGES=1000       # Maximum pages to crawl
CRAWLER_MAX_DEPTH=3          # Maximum crawl depth
CRAWLER_RESPECT_ROBOTS=true  # Respect robots.txt

# Content Settings
CRAWLER_MAX_CONTENT=50000    # Max content length
CRAWLER_MIN_CONTENT=100      # Min content length
CRAWLER_SNIPPET_LENGTH=300   # Snippet length
CRAWLER_KEYWORD_LIMIT=20     # Max keywords per page

# Logging
LOG_LEVEL=info              # debug|info|silent
CRAWLER_CACHE_DIR=./crawler-cache
```

## üìñ Usage Examples

### 1. News Website Crawling

```bash
# Crawl major news sites
node web-crawler.js \
  "https://techcrunch.com/sitemap.xml" \
  "https://www.bbc.com/news" \
  "https://arstechnica.com"
```

### 2. Blog Content Crawling

```bash
# Crawl AI/ML blogs
node web-crawler.js \
  "https://blog.openai.com" \
  "https://ai.googleblog.com" \
  "https://www.deepmind.com/blog"
```

### 3. Documentation Crawling

```bash
# Crawl technical documentation
node web-crawler.js \
  "https://docs.python.org/3/" \
  "https://nodejs.org/en/docs/" \
  "https://reactjs.org/docs/"
```

### 4. E-commerce Product Crawling

```bash
# Set longer delays for e-commerce
export CRAWLER_DELAY=3000
export CRAWLER_CONCURRENT=2

node web-crawler.js \
  "https://example-store.com/sitemap.xml"
```

### 5. Batch Processing from File

```bash
# Create URL file
cat > urls.txt << EOF
https://example.com/page1
https://example.com/page2
https://example.com/page3
https://another-site.com/article1
https://another-site.com/article2
EOF

# Crawl from file
node web-crawler.js --file urls.txt
```

### 6. Custom Configuration

```bash
# High-performance crawling
export CRAWLER_CONCURRENT=10
export CRAWLER_DELAY=500
export CRAWLER_MAX_PAGES=5000
export LOG_LEVEL=debug

node web-crawler.js "https://large-site.com/sitemap.xml"
```

## üîß Advanced Usage

### Programmatic Usage

```javascript
const WebCrawler = require('./web-crawler');

async function crawlWebsite() {
  const crawler = new WebCrawler();
  
  const urls = [
    'https://example.com',
    'https://blog.example.com'
  ];
  
  try {
    const stats = await crawler.crawl(urls);
    console.log('Crawling completed:', stats);
  } catch (error) {
    console.error('Crawling failed:', error.message);
  }
}

crawlWebsite();
```

### Custom Content Extraction

```javascript
const WebCrawler = require('./web-crawler');
const { getConfig } = require('./crawler-config');

// Get configuration for specific site
const config = getConfig('https://reddit.com/r/programming');
console.log('Reddit config:', config);

// Custom extraction logic
const crawler = new WebCrawler();
const customExtract = crawler.extractContent(html, url);
```

### Monitoring and Analytics

```javascript
const crawler = new WebCrawler();

// Monitor progress
crawler.on('progress', (stats) => {
  console.log(`Processed: ${stats.processed}, Indexed: ${stats.indexed}`);
});

// Handle errors
crawler.on('error', (error, url) => {
  console.log(`Error crawling ${url}: ${error.message}`);
});
```

## üìä Output Format

Each crawled page is stored as a JSON document in Elasticsearch:

```json
{
  "title": "Example Article Title",
  "url": "https://example.com/article",
  "snippet": "This is a brief description of the article content...",
  "keywords": ["technology", "AI", "machine learning"],
  "content": "Full article content with main text extracted...",
  "source": "example.com",
  "domain": "example.com",
  "contentType": "article",
  "language": "en",
  "publishDate": "2024-01-15T10:30:00Z",
  "lastModified": "2024-01-15T10:30:00Z",
  "crawledAt": "2024-01-15T12:00:00Z",
  "contentHash": "a1b2c3d4e5f6...",
  "wordCount": 1250,
  "readingTime": 7,
  "metadata": {
    "author": "John Doe",
    "description": "Article meta description",
    "image": "https://example.com/image.jpg",
    "canonical": "https://example.com/article",
    "robots": "index,follow"
  }
}
```

## üß™ Testing

### Run Test Suite

```bash
# Basic tests
node test-crawler.js

# Include network tests
node test-crawler.js --network

# Or with environment variable
RUN_NETWORK_TESTS=true node test-crawler.js
```

### Test Coverage

The test suite covers:
- ‚úÖ Content extraction and parsing
- ‚úÖ Robots.txt compliance
- ‚úÖ URL filtering and validation
- ‚úÖ Content type detection
- ‚úÖ Configuration loading
- ‚úÖ Text cleaning and normalization
- ‚úÖ Incremental crawling logic
- ‚úÖ Sitemap parsing
- ‚úÖ Error handling and retry logic
- ‚úÖ File input processing
- ‚úÖ Memory usage and cleanup
- ‚úÖ Concurrency limits

### Example Test Run

```bash
$ node test-crawler.js

üöÄ Starting Web Crawler Test Suite

üß™ Running test: Content Extraction
‚úÖ PASSED: Content Extraction

üß™ Running test: Robots.txt Checking
‚úÖ PASSED: Robots.txt Checking

üß™ Running test: URL Filtering
‚úÖ PASSED: URL Filtering

# ... more tests ...

üìä Test Results Summary
==================================================
Total Tests: 12
Passed: 12
Failed: 0
Success Rate: 100.0%

‚úÖ All tests passed!
```

## üîß Configuration

### Site-Specific Configurations

The crawler includes optimized configurations for popular websites:

- **Reddit**: Score extraction, comment handling
- **Stack Overflow**: Tags, votes, accepted answers
- **GitHub**: Repository metadata, language detection
- **Medium**: Author, claps, publish date
- **YouTube**: Video metadata, duration, views

### Content Type Detection

Automatic detection for:
- **News**: Breaking news, articles, reports
- **Blog**: Personal blogs, company blogs
- **Product**: E-commerce, product pages
- **Documentation**: API docs, guides, tutorials
- **Video**: YouTube, Vimeo, video content
- **Academic**: Research papers, journals

### Language Detection

Supports detection of:
- English, Spanish, French, German
- Italian, Portuguese, Russian
- Chinese, Japanese, Korean
- And more via HTML lang attributes

## üìà Performance

### Benchmarks

- **Speed**: 5-50 pages/second (depending on configuration)
- **Memory**: ~50MB for 1000 pages
- **Concurrency**: Up to 20 concurrent requests
- **Efficiency**: 95%+ success rate on well-formed sites

### Optimization Tips

1. **Adjust Concurrency**: Higher for fast sites, lower for slow sites
2. **Set Appropriate Delays**: Respect server load and robots.txt
3. **Use Incremental Crawling**: Only process changed content
4. **Filter URLs**: Skip unnecessary file types and admin pages
5. **Monitor Memory**: Use streaming for very large crawls

## üõ°Ô∏è Best Practices

### Ethical Crawling

1. **Respect robots.txt**: Always check and follow robots.txt rules
2. **Rate Limiting**: Don't overwhelm servers with requests
3. **User Agent**: Use descriptive, identifiable user agent
4. **Legal Compliance**: Respect copyright and terms of service
5. **Data Privacy**: Handle personal data responsibly

### Production Deployment

1. **Error Monitoring**: Use Sentry or similar for error tracking
2. **Logging**: Implement structured logging for debugging
3. **Metrics**: Monitor crawl rates, success rates, and errors
4. **Scaling**: Use distributed crawling for large-scale operations
5. **Storage**: Implement proper backup and retention policies

## üîç Elasticsearch Integration

### Index Mapping

The crawler creates an optimized Elasticsearch index with:
- **Text Analysis**: Custom analyzers for content and keywords
- **Vector Support**: Ready for semantic search integration
- **Faceted Search**: Support for filtering by source, type, language
- **Full-text Search**: Optimized for search relevance

### Query Examples

```javascript
// Search for content
GET /web-content/_search
{
  "query": {
    "multi_match": {
      "query": "artificial intelligence",
      "fields": ["title^2", "content", "keywords"]
    }
  }
}

// Filter by content type
GET /web-content/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "content": "machine learning" } }
      ],
      "filter": [
        { "term": { "contentType": "article" } }
      ]
    }
  }
}

// Aggregate by source
GET /web-content/_search
{
  "size": 0,
  "aggs": {
    "sources": {
      "terms": { "field": "source" }
    }
  }
}
```

## üö® Troubleshooting

### Common Issues

#### 1. Connection Errors
```bash
Error: connect ECONNREFUSED 127.0.0.1:9200
```
**Solution**: Check Elasticsearch is running and URL is correct

#### 2. Rate Limiting
```bash
Error: Request failed with status code 429
```
**Solution**: Increase delay between requests, reduce concurrency

#### 3. Memory Issues
```bash
Error: JavaScript heap out of memory
```
**Solution**: Reduce batch size, increase Node.js memory limit

#### 4. Robots.txt Blocking
```bash
Blocked by robots.txt
```
**Solution**: Check robots.txt, adjust user agent, or disable robots checking

### Debug Mode

```bash
# Enable debug logging
export LOG_LEVEL=debug
node web-crawler.js "https://example.com"
```

### Health Checks

```bash
# Check Elasticsearch connection
curl http://localhost:9200/_cluster/health

# Check index status
curl http://localhost:9200/web-content/_stats

# View recent documents
curl http://localhost:9200/web-content/_search?size=5&sort=crawledAt:desc
```

## üìö API Reference

### WebCrawler Class

#### Constructor
```javascript
const crawler = new WebCrawler();
```

#### Methods

##### `crawl(input)`
Main crawling method that accepts:
- `string`: Single URL or sitemap URL
- `Array<string>`: Multiple URLs
- `{file: string}`: Path to URL file

Returns: `Promise<CrawlStats>`

##### `crawlUrl(url, depth)`
Crawl a single URL with specified depth.

##### `extractContent(html, url)`
Extract structured content from HTML.

##### `checkRobots(url)`
Check if URL is allowed by robots.txt.

##### `hasContentChanged(url, contentHash)`
Check if content has changed since last crawl.

### Configuration Functions

#### `getConfig(url, contentType)`
Get site-specific configuration.

#### `detectContentType(url)`
Detect content type from URL patterns.

#### `shouldSkipUrl(url)`
Check if URL should be skipped.

## ü§ù Contributing

### Development Setup

```bash
# Clone repository
git clone <repository-url>
cd scripts/crawler

# Install dependencies
npm install

# Run tests
npm test

# Run examples
node examples/crawl-examples.js
```

### Adding New Site Configurations

1. Edit `crawler-config.js`
2. Add site-specific selectors
3. Test with sample URLs
4. Update documentation

### Reporting Issues

Please include:
- URL being crawled
- Error message
- Environment details
- Configuration used

## üìÑ License

This web crawler is part of the AI-Powered Search Engine project and is licensed under the MIT License.

## üîó Related Tools

- **[AI Search Engine](../../README.md)**: Main search engine application
- **[Elasticsearch Optimization](../config/elasticsearch-optimized.ts)**: Index configuration
- **[Docker Setup](../../docker-compose.yml)**: Containerized deployment
- **[Deployment Guide](../../DEPLOYMENT.md)**: Production deployment instructions

---

**Built with ‚ù§Ô∏è for the AI-Powered Search Engine project**
